'''
Метод градиентного спуска. Итерационный алгоритм минимизации функции потерь (по умолчанию hinge loss function).
Алгоритм также применяется в задачах прогнозирования.
Также есть версия стохастического градиентного спуска, который применяется при больших выборках.
Его суть в том что он считает производную не по всей выборке, а по каждому наблюдению (online learning) (или по группе наблюдений mini-batch) и меняет веса.
В итоге он приходит в тот же оптимум что и при обычном ГС. Существуют методы применения ГС для МНК, логит, тобит и других методов (доказательства).
Сильные стороны: высокая точность классификации и прогнозирования, подходит для мультиклассовой классификации.
Слабые стороны — чувствительность к параметрам модели.
'''
from sklearn.linear_model import SGDClassifier

#model fit
X = [[0., 0.], [1., 1.]]
y = [0, 1]
clf = SGDClassifier(loss="hinge", penalty="l2", max_iter=5)
clf.fit(X, y)

#result
print(clf.predict([[2., 2.]]))
print(clf.coef_)
print(clf.intercept_)