''''Почему Tensorflow?
Исследования показали, что TensorFlow — эффективная и масштабируемая платформа для задач глубокого обучения. Исследование, 
проведённое сотрудниками Google, показало, что TensorFlow превзошёл другие популярные фреймворки глубокого обучения, 
достигнув более высокой производительности и более быстрого обучения в крупномасштабных нейронных сетях.
Другой эксперимент, проведённый исследователями из Массачусетского технологического института и Калифорнийского 
университета в Беркли, показал, что TensorFlow — это универсальный инструмент для разработки и развёртывания моделей 
глубокого обучения в самых разных приложениях.

Импорт TensorFlow и проверка версии
Прежде чем приступить к работе с TensorFlow, вам необходимо импортировать библиотеку и убедиться, что у вас установлена 
правильная версия. Этот процесс включает в себя использование оператора import и атрибута __version__.'''''

# Importing TensorFlow and checking the version
import tensorflow as tf
print('TensorFlow version:', tf.__version__)

''''В этом коде мы импортируем TensorFlow с помощью оператора import и проверяем версию, благодаря атрибуту __version__.

Построение последовательной модели в TensorFlow
В TensorFlow вы можете построить последовательную модель, создав экземпляр объекта Sequential и добавив к нему слои с 
помощью метода add(). Этот процесс включает в себя определение архитектуры модели, в том числе входные и выходные формы, 
функции активации и количество единиц в каждом слое.'''''

# Building a sequential model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dense(10, activation='softmax')
])

''''В этом коде мы создаём последовательную модель с двумя плотными слоями, используя объект Sequential и класс слоя Dense. 
Мы определяем входную форму первого слоя как кортеж (784,), который соответствует плоской форме набора данных MNIST. 
Также мы указываем функцию активации для каждого слоя ( relu для первого слоя и softmax для выходного слоя), а также 
количество единиц в каждом слое ( 64 для первого слоя и 10 для выходного слоя).

Компиляция модели в TensorFlow
После построения модели в TensorFlow перед обучением её необходимо скомпилировать. Этот процесс включает в себя указание 
функции потерь, оптимизатора и метрики оценки для модели с использованием метода compile().'''''

# Compiling the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

''''В этом коде мы компилируем последовательную модель с помощью метода compile() и указываем функцию потерь 
categorical_crossentropy, оптимизатор adam и метрику accuracy для оценки во время обучения.

Обучение модели в TensorFlow
Чтобы обучить модель в TensorFlow, вам нужно вызвать метод fit() на скомпилированной модели и передать данные обучения 
и проверки, а также размер пакета и количество эпох.'''''

# Training the model
model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val))

''''В этом коде мы обучаем скомпилированную модель с помощью метода fit() и передаём обучающие данные ( X_train и y_train), 
размер партии ( 32), количество эпох ( 10) и данные проверки ( X_val и y_val). Модель будет обучаться в течение 10 эпох 
с размером пакета 32 и проверять данные проверки после каждой эпохи.

Оценка модели в TensorFlow
После того, как вы обучили модель в TensorFlow, вы можете оценить её производительность на тестовом наборе с помощью 
метода evaluate(). Этот процесс включает в себя передачу тестовых данных в качестве аргументов и вычисление метрики потерь 
и оценки в тестовом наборе.'''''

# Evaluating the model
loss, accuracy = model.evaluate(X_test, y_test)
print('Test Loss:', loss)
print('Test Accuracy:', accuracy)

''''В этом коде мы оцениваем обученную модель с помощью метода evaluate() и передаём тестовые данные ( X_test и y_test). 
Затем мы вычисляем метрики потерь и точности в тестовом наборе и выводим их.

Сохранение и загрузка модели в TensorFlow
В TensorFlow вы можете сохранить обученную модель на диск с помощью метода save()и загрузить её обратно в память с помощью 
функции load_model(). Это позволяет повторно использовать обученную модель в будущих сеансах без необходимости переобучения её с нуля.'''''

# Saving the model to disk
model.save('my_model.h5')
# Loading the model from disk
new_model = tf.keras.models.load_model('my_model.h5')

''''В этом коде мы сохраняем обученную модель на диск с помощью метода save()и загружаем её обратно в память с помощью 
функции load_model(). Мы сохраняем модель как файл HDF5 с именем my_model.h5 и загружаем её обратно в новую переменную 
с именем new_model.

Использование обратных вызовов в TensorFlow
В TensorFlow вы можете использовать обратные вызовы для настройки поведения процесса обучения. Обратные вызовы — это 
функции, которые выполняются во время обучения в определённых точках, таких как начало или конец эпохи. Их можно 
использовать для таких задач, как сохранение модели, ранняя остановка или снижение скорости обучения.'''''

# Defining a callback function
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)
# Training the model with the callback function
model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val), callbacks=[callback])

''''Объяснение: В этом коде мы определяем функцию обратного вызова, которая выполняет досрочную остановку, если потеря 
проверки не улучшится после трёх эпох. Мы создаём экземпляр обратного вызова EarlyStopping и передаём его методу fit() 
в виде списка обратных вызовов. Модель будет обучаться в течение 10 эпох и остановится раньше, если потеря проверки не 
улучшится после трёх эпох.

Использование регуляризации в TensorFlow
В TensorFlow вы можете использовать методы регуляризации, такие как L1, L2 и отсев, чтобы предотвратить переобучение в 
ваших моделях. Регуляризация включает в себя добавление штрафов к функции потерь, которые препятствуют использованию 
больших весов или активаций в модели.'''''

# Adding L2 regularization to a dense layer
model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))

''''В этом коде мы добавляем регуляризацию L2 к плотному слою модели, передавая аргумент kernel_regularizer конструктору 
слоя Dense. Указываем силу регуляризации использования 0.01 функции l2() из модуля tf.keras.regularizers.

Использование свёрточных слоев в TensorFlow
В TensorFlow вы можете использовать свёрточные слои для извлечения признаков из изображений и других многомерных данных. 
Свёрточные слои состоят из фильтров, которые скользят по входным данным и создают карту объектов.'''''

# Adding a 2D convolutional layer
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
# Adding a 2D max pooling layer
model.add(tf.keras.layers.MaxPooling2D((2, 2)))

''''В этом коде мы добавляем к модели двухмерный свёрточный слой, используя класс Conv2D слоя. Мы указываем фильтры 
размера (3, 3), активации relu и входной формы (28, 28, 1)(соответствующей набору данных MNIST). Затем мы добавляем слой 
пула 2D max, используя класс слоя MaxPooling2D с размером пула (2, 2).

Использование повторяющихся слоёв в TensorFlow
В TensorFlow вы можете использовать повторяющиеся слои для моделирования последовательных данных, таких как временные 
ряды или естественный язык. Рекуррентные слои состоят из рекуррентных блоков, которые обрабатывают каждый элемент в 
последовательности и создают выходное и скрытое состояние, которые передаются следующему блоку.'''''

# Adding a simple RNN layer
model.add(tf.keras.layers.SimpleRNN(64, activation='relu', input_shape=(None, 1)))
# Adding a LSTM layer
model.add(tf.keras.layers.LSTM(64, activation='relu'))

''''В этом коде мы добавляем в модель слой простой рекуррентной нейронной сети (RNN), используя класс слоя SimpleRNN . 
Мы указываем 64 повторяющиеся единицы, активацию relu и входную форму (None, 1)(где None представляет любую длину 
последовательности). Затем мы добавляем слой долговременной кратковременной памяти (LSTM), используя класс словя LSTM 
с теми же параметрами.

Использование трансферного обучения в TensorFlow
Трансферное обучение — это метод, в котором предварительно обученная модель используется в качестве отправной точки для 
новой задачи. В TensorFlow вы можете использовать предварительно обученные модели из таких источников, как TensorFlow 
Hub или модуль приложений Keras, для выполнения трансферного обучения.'''''

# Loading a pre-trained model
base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')

# Freezing the pre-trained layers
for layer in base_model.layers:
    layer.trainable = False
# Adding new output layers for a new task
x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)
x = tf.keras.layers.Dense(128, activation='relu')(x)
predictions = tf.keras.layers.Dense(10, activation='softmax')(x)
# Creating a new model
model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)

''''В этом коде мы загружаем предварительно обученную модель MobileNetV2 из модуля Keras Applications с входной формой 
(224, 224, 3)верхнего уровня классификации и без него.
Затем мы заморозим предварительно обученные слои, установив для их атрибута trainable значение False.
Мы добавляем новые выходные слои для новой задачи, применяя глобальное усреднение к выходным данным базовой модели, 
добавляя плотный слой с 128 единицами и активацией relu и добавляя плотный слой с 10 единицами и активацией softmax. 
Наконец, мы создаём новую модель, которая принимает входные данные базовой модели и выводит новые прогнозы.
Использование пользовательских функций потерь в TensorFlow
В TensorFlow вы можете определить собственные функции потерь для своих моделей с помощью модуля tf.keras.losses. Это 
позволяет вам создавать функции потерь, адаптированные к вашей конкретной проблеме, которые могут включать дополнительные 
условия или штрафы.'''''

# Defining a custom loss function
def custom_loss(y_true, y_pred):
    squared_difference = tf.square(y_true - y_pred)
    return tf.reduce_mean(squared_difference, axis=-1)

# Compiling the model with the custom loss function
model.compile(optimizer='adam', loss=custom_loss)

''''В этом коде мы определяем пользовательскую функцию потерь custom_loss, которая вычисляет среднеквадратичную ошибку 
между истинным и прогнозируемым значениями. Мы используем операции TensorFlow для вычисления квадрата разницы и уменьшения 
среднего по последней оси. Затем мы компилируем модель с оптимизатором adam и пользовательской функцией потерь.

Использование пользовательских метрик в TensorFlow
В TensorFlow вы можете определить пользовательские метрики для оценки производительности ваших моделей во время обучения 
и тестирования. Пользовательские показатели могут быть определены с помощью модуля tf.keras.metrics и могут включать 
дополнительные условия или штрафы.'''''

# Defining a custom metric function
def custom_metric(y_true, y_pred):
    absolute_difference = tf.abs(y_true - y_pred)
    return tf.reduce_mean(absolute_difference, axis=-1)

# Compiling the model with the custom metric
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[custom_metric])

''''В этом коде мы определяем пользовательскую метрику custom_metric, которая вычисляет среднюю абсолютную ошибку между 
истинным и прогнозируемым значениями. Мы используем операции TensorFlow для вычисления абсолютной разницы и уменьшения 
среднего по последней оси. Затем мы компилируем модель с оптимизатором adam, разреженной категориальной функцией 
кросс-энтропийных потерь и пользовательской метрикой.

Использование функционального API в TensorFlow
Функциональный API Keras позволяет создавать сложные модели с несколькими входами и выходами, общими слоями и нелинейными 
связями между слоями.'''''

# Defining the inputs
input_1 = tf.keras.layers.Input(shape=(28, 28, 1))
input_2 = tf.keras.layers.Input(shape=(10,))

# Adding some layers to the first input
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(input_1)
x = tf.keras.layers.MaxPooling2D((2, 2))(x)
x = tf.keras.layers.Flatten()(x)

# Merging the inputs with a shared layer
merged = tf.keras.layers.concatenate([x, input_2])

# Adding some more layers
y = tf.keras.layers.Dense(64, activation='relu')(merged)
output = tf.keras.layers.Dense(10, activation='softmax')(y)

# Creating the model
model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=output)

''''В этом коде мы определяем два входа для модели: input_1 для данных изображения и input_2 для дополнительных функций.
Мы добавляем несколько слоёв к первому входу для обработки данных изображения, включая слой свёртки 2D, слой максимального 
объединения и слой сглаживания.

Затем мы объединяем выходные данные этих слоёв со вторым входом, используя конкатенированный слой.
Мы добавляем ещё несколько слоёв к объединённому выводу, в том числе плотный слой с 64 единицами и активацией relu и 
плотный выходной слой с 10 единицами и активацией softmax. Наконец, мы создаём модель с классом Model, используя 
входные и выходные данные.

Использование обратных вызовов в TensorFlow
В TensorFlow вы можете использовать обратные вызовы для настройки поведения ваших моделей во время обучения. 
Обратные вызовы — это функции, которые вызываются в определённые моменты во время обучения, например, в конце каждой эпохи, 
и могут выполнять такие задачи, как сохранение модели, ранняя остановка или изменение скорости обучения.'''''

# Defining a callback for early stopping
early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)
# Fitting the model with callbacks
history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val), callbacks=[early_stopping_callback])

''''В этом коде мы определяем обратный вызов, вызываемый early_stopping_callback с использованием класса EarlyStopping 
из модуля tf.keras.callbacks. Этот обратный вызов будет отслеживать потерю проверки и останавливать обучение, если оно 
не улучшается в течение 3 последовательных эпох. Затем мы подгоняем модель к обучающим данным для 10 эпох с включёнными 
проверочными данными и обратным вызовом. Переменная history будет содержать информацию о производительности обучения и 
проверки с течением времени.'''''